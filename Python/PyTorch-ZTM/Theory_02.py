#!/usr/bin/env python
# coding: utf-8

# # Crross Entropy Loss
# 
# https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e

# # Dropout
# 
# Dropout is used to make the model not over fit and it reduces the number of nodes used or essentially deactivates them
# 
# By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections
# 
# Dilution and dropout are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Wikipedia
# 
# https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf

# # Batch normalization
# 
# Another way to prevent model overfitting
# 
# Normalization is a data preprocessing too used
# 
# When we normalise the model we can generalize its weights and biases
# 
# The batch normalization is kind of a filter where it blends up all of the information
# 
# https://arxiv.org/pdf/1502.03167.pdf

# In[3]:





# In[7]:





# In[ ]:




