{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(3)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = 'PetImages/Dog'\n",
    "    LABELS = {CATS:0,DOGS:1}\n",
    "    training_data = []\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "    \n",
    "    def make_training_data(self):\n",
    "        for label in tqdm(self.LABELS):\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try:\n",
    "                    path = f\"{label}/{f}\"\n",
    "                    img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img),np.eye(2)[self.LABELS[label]]])\n",
    "                    if label == self.CATS:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        for _ in range(50):\n",
    "            np.random.shuffle(self.training_data)\n",
    "        np.save('training_data.npy',self.training_data)\n",
    "        print(f'Cats : {self.catcount}')\n",
    "        print(f'Dogs : {self.dogcount}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_DATA:\n",
    "        dvc = DogsVSCats()\n",
    "        dvc.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('training_data.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24946"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[44, 47, 46, ...,  5, 51, 52],\n",
      "       [45, 50, 69, ...,  5, 54, 55],\n",
      "       [42, 43, 48, ...,  8, 57, 54],\n",
      "       ...,\n",
      "       [27, 31, 98, ..., 34, 34, 34],\n",
      "       [74, 93, 99, ..., 32, 32, 31],\n",
      "       [84, 90, 93, ..., 30, 28, 29]], dtype=uint8)\n",
      " array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 65,  63,  61, ...,  15,  16,  14],\n",
      "       [ 67,  66,  64, ...,  32, 175, 174],\n",
      "       [ 73,  68,  67, ...,  31, 229, 230],\n",
      "       ...,\n",
      "       [ 36,  10,  17, ..., 116, 127, 129],\n",
      "       [ 21,  21,  22, ...,  82, 122, 128],\n",
      "       [ 19,  31,  57, ...,  19,  77, 120]], dtype=uint8)\n",
      " array([1., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(training_data[1][0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "        self.fc1 = nn.Linear(128 * 5 * 5,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "        print(x.shape)\n",
    "        x = x.view(-1,128*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3200, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_funtion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training_data:\n",
    "    X.append(i[0]/255.0)\n",
    "    y.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24946"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24946"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.view(-1,50,50) # 50,50 is the img size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6236\n"
     ]
    }
   ],
   "source": [
    "VAl_PCT = 0.25\n",
    "val_size = int(len(X)*VAl_PCT)\n",
    "print(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:-val_size]\n",
    "y_train = y[:-val_size]\n",
    "X_test = X[-val_size:]\n",
    "y_test = X[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18710\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6236\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3200]' is invalid for input of size 250000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-303ea6492b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print(f\"{i}:{i+BATCH_SIZE}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3200]' is invalid for input of size 250000"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPCOHS = 100\n",
    "for epoch in range(EPCOHS):\n",
    "    for i in tqdm(range(0, len(X_train), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "        #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "        batch_X = X_train[i:i+BATCH_SIZE].view(-1,128*5*5)\n",
    "        batch_y = y_train[i:i+BATCH_SIZE]\n",
    "        net.zero_grad()\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_funtion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i])\n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "\n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "24946\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1  # lets reserve 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i])\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  # returns a list, \n",
    "            predicted_class = torch.argmax(net_out)\n",
    "\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.19646070897579193\n",
      "Epoch: 1. Loss: 0.1471402645111084\n",
      "Epoch: 2. Loss: 0.11289826780557632\n",
      "Epoch: 3. Loss: 0.1060304194688797\n",
      "Epoch: 4. Loss: 0.08537285774946213\n",
      "Epoch: 5. Loss: 0.06543461978435516\n",
      "Epoch: 6. Loss: 0.035588450729846954\n",
      "Epoch: 7. Loss: 0.07862048596143723\n",
      "Epoch: 8. Loss: 0.020282069221138954\n",
      "Epoch: 9. Loss: 0.014307864010334015\n",
      "Epoch: 10. Loss: 0.037449803203344345\n",
      "Epoch: 11. Loss: 0.00981384888291359\n",
      "Epoch: 12. Loss: 0.03204044699668884\n",
      "Epoch: 13. Loss: 0.037781450897455215\n",
      "Epoch: 14. Loss: 0.03460770845413208\n",
      "Epoch: 15. Loss: 0.0028460314497351646\n",
      "Epoch: 16. Loss: 0.040707387030124664\n",
      "Epoch: 17. Loss: 0.019251180812716484\n",
      "Epoch: 18. Loss: 0.02086319401860237\n",
      "Epoch: 19. Loss: 0.04464838281273842\n",
      "Epoch: 20. Loss: 0.00046379482955671847\n",
      "Epoch: 21. Loss: 0.001634777756407857\n",
      "Epoch: 22. Loss: 0.0007858287426643074\n",
      "Epoch: 23. Loss: 0.00839089136570692\n",
      "Epoch: 24. Loss: 0.021580969914793968\n",
      "Epoch: 25. Loss: 0.001725488342344761\n",
      "Epoch: 26. Loss: 0.0005176184931769967\n",
      "Epoch: 27. Loss: 0.007948338985443115\n",
      "Epoch: 28. Loss: 0.00038039570790715516\n",
      "Epoch: 29. Loss: 0.003515078453347087\n",
      "Epoch: 30. Loss: 0.0010746776824817061\n",
      "Epoch: 31. Loss: 0.027108853682875633\n",
      "Epoch: 32. Loss: 4.1484969415250816e-07\n",
      "Epoch: 33. Loss: 0.04859830066561699\n",
      "Epoch: 34. Loss: 1.2930416232848074e-05\n",
      "Epoch: 35. Loss: 4.30208419857081e-05\n",
      "Epoch: 36. Loss: 0.00225369562394917\n",
      "Epoch: 37. Loss: 3.944646778109018e-06\n",
      "Epoch: 38. Loss: 3.0056828563829185e-06\n",
      "Epoch: 39. Loss: 5.4036809160606936e-06\n",
      "Epoch: 40. Loss: 4.5967776713951025e-06\n",
      "Epoch: 41. Loss: 0.0002656964061316103\n",
      "Epoch: 42. Loss: 0.0042561437003314495\n",
      "Epoch: 43. Loss: 4.385798456496559e-05\n",
      "Epoch: 44. Loss: 0.021247623488307\n",
      "Epoch: 45. Loss: 1.8673263824098285e-08\n",
      "Epoch: 46. Loss: 5.120424248161726e-06\n",
      "Epoch: 47. Loss: 1.2825553952211521e-08\n",
      "Epoch: 48. Loss: 0.0001807304797694087\n",
      "Epoch: 49. Loss: 2.5448911401326768e-05\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "def train(net):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 250\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE): # from 0, to the len of x, stepping BATCH_SIZE at a time. [:50] ..for now just to dev\n",
    "            #print(f\"{i}:{i+BATCH_SIZE}\")\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            net.zero_grad()\n",
    "\n",
    "            optimizer.zero_grad()   # zero the gradient buffers\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()    # Does the update\n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.to(device)\n",
    "test_y.to(device)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  # returns a list, \n",
    "            predicted_class = torch.argmax(net_out)\n",
    "\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in tqdm(range(0, len(test_X), BATCH_SIZE)):\n",
    "\n",
    "    batch_X = test_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50).to(device)\n",
    "    batch_y = test_y[i:i+BATCH_SIZE].to(device)\n",
    "    batch_out = net(batch_X)\n",
    "\n",
    "    out_maxes = [torch.argmax(i) for i in batch_out]\n",
    "    target_maxes = [torch.argmax(i) for i in batch_y]\n",
    "    for i,j in zip(out_maxes, target_maxes):\n",
    "        if i == j:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python373jvsc74a57bd0210f9608a45c0278a93c9e0b10db32a427986ab48cfc0d20c139811eb78c4bbc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
