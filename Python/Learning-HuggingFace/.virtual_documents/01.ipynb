


from transformers import pipeline


classifier = pipeline("sentiment-analysis")


classifier("I've been waiting for a HuggingFace Course my whole life")


classifier([
    "I've been waiting for a HuggingFace Course my whole life",
    "I hate this so much!"
])


classifier = pipeline('zero-shot-classification')
classifier(
    "This is a course about the Transformers library",
    candidate_labels=['education', 'politics', 'buisness']
)


generator = pipeline('text-generation')
generator("In this course, we will teach you how to")


generator = pipeline('text-generation', model="distilgpt2")
generator("In this course, we will teach you how to", max_length=30, num_return_sequences=2) # no. of words and no. of like generated texts


unmasker = pipeline('fill-mask')
unmasker('This course will teach you all about <mask> modoels', top_k=2)


ner = pipeline('ner')
ner("My name is Sylavin and I work at Hugging Face in Brookyln")


question_answerer = pipeline("question-answering")
question_answerer(question="Where do I work?", context="My name is Sylavin and I work at Hugging Face in Brooklyn")


summarizer = pipeline("summarization")
summarizer("A critical phase in Roman history was the Punic Wars against Carthage (264–146 BC). The First Punic War (264–241 BC) established Rome as a naval power, while the Second Punic War (218–201 BC) saw the rise of Hannibal, whose daring crossing of the Alps remains legendary. Despite early Carthaginian victories, Rome eventually triumphed, culminating in the Third Punic War (149–146 BC), which led to the complete destruction of Carthage.")


translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face")





from transformers import AutoTokenizer


checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!"
]


inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")


inputs


from transformers import AutoModel


checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)


outputs = model(**inputs)


print(outputs.last_hidden_state.shape)


from transformers import AutoModelForSequenceClassification


checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)


outputs = model(**inputs)


outputs.logits


import torch
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)


predictions


model.config.id2label





from transformers import AutoModel


bert_model = AutoModel.from_pretrained("bert-base-cased")
bert_config = AutoModel.from_pretrained("bert-base-cased")


gpt_model = AutoModel.from_pretrained("gpt2")
gpt_config = AutoModel.from_pretrained("gpt2")


bart_model = AutoModel.from_pretrained("facebook/bart-base")
bart_config = AutoModel.from_pretrained("facebook/bart-base")


from transformers import BertConfig, BertModel
bert_config = BertConfig.from_pretrained("bert-base-cased")
bert_model = BertModel(bert_config)


















